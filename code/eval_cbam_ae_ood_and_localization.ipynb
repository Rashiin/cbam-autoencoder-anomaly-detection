{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6da5805",
   "metadata": {},
   "source": [
    "# One-Class Anomaly Detection on CIFAR-10 (ConvAE + CBAM)\n",
    "\n",
    "This project studies **one-class / unsupervised anomaly detection** on images using a\n",
    "**Convolutional Autoencoder (ConvAE)** trained only on *normal* samples from CIFAR-10.\n",
    "An attention mechanism (**CBAM: Channel & Spatial Attention**) is integrated to improve\n",
    "feature focusing and provide more interpretable anomaly cues.\n",
    "\n",
    "## Key Idea\n",
    "- Train an autoencoder **only on one normal class** (default: `airplane`).\n",
    "- At test time:\n",
    "  - **OOD detection**: treat other CIFAR-10 classes as anomalies (image-level).\n",
    "  - **Synthetic localization**: inject synthetic anomalies into normal images (pixel-level masks).\n",
    "\n",
    "## Model\n",
    "- Encoderâ€“Decoder ConvAE\n",
    "- CBAM inserted at the bottleneck:\n",
    "  - Channel Attention\n",
    "  - Spatial Attention (also used as a soft weighting for anomaly scoring)\n",
    "\n",
    "## Outputs\n",
    "When you run evaluation, the script prints:\n",
    "- `AUROC recon_mean`\n",
    "- `AUROC recon_att_weighted`\n",
    "- `AUROC fused(rank)`\n",
    "- Localization metrics on synthetic anomalies:\n",
    "  - IoU/Dice with fixed threshold (0.5)\n",
    "  - IoU/Dice with quantile threshold\n",
    "\n",
    "It also saves:\n",
    "- `./runs/vis_final.png` (qualitative visualization grid)\n",
    "\n",
    "## Requirements\n",
    "- Python 3.9+\n",
    "- PyTorch + torchvision\n",
    "- matplotlib\n",
    "- numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d7aa2",
   "metadata": {},
   "source": [
    "One-Class Image Anomaly Detection using ConvAE + CBAM\n",
    "\n",
    "This script implements an unsupervised / one-class anomaly detection\n",
    "framework on the CIFAR-10 dataset.\n",
    "\n",
    "Core ideas:\n",
    "- Train a convolutional autoencoder (ConvAE) using only normal samples.\n",
    "- Integrate CBAM (Channel & Spatial Attention) at the bottleneck layer.\n",
    "- Detect anomalies via reconstruction error.\n",
    "- Enhance anomaly scoring using attention-weighted reconstruction.\n",
    "- Evaluate both:\n",
    "  (1) Image-level anomaly detection (AUROC on OOD samples)\n",
    "  (2) Pixel-level anomaly localization using synthetic anomalies\n",
    "\n",
    "Key Features:\n",
    "- One-class training (no anomaly labels used in training)\n",
    "- Synthetic anomaly generation for localization evaluation\n",
    "- Attention-aware anomaly scoring\n",
    "- CPU-friendly evaluation setup\n",
    "- Qualitative visualization of anomaly maps and masks\n",
    "\n",
    "Outputs:\n",
    "- AUROC scores (mean, attention-weighted, fused)\n",
    "- IoU / Dice scores for localization\n",
    "- Visualization image saved to ./runs/vis_final.png\n",
    "\n",
    "Dataset:\n",
    "- CIFAR-10 (single normal class, default: airplane)\n",
    "\n",
    "Author:\n",
    "- Rashin Gholijani Farahani\n",
    "\n",
    "Course:\n",
    "- Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005d7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15669609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d6a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Config\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # Paths\n",
    "    root: str = \"./data\"\n",
    "    save_dir: str = \"./runs\"\n",
    "    ckpt_name: str = \"best_cpu_fast.pt\"\n",
    "\n",
    "    # One-class setup\n",
    "    normal_class: int = 0\n",
    "    seed: int = 42\n",
    "\n",
    "    # Model\n",
    "    base_ch: int = 24\n",
    "    dropout_p: float = 0.10\n",
    "\n",
    "    # Data loading\n",
    "    batch_train: int = 32\n",
    "    batch_test: int = 128\n",
    "    num_workers: int = 0\n",
    "    torch_threads: int = 2\n",
    "\n",
    "    # Evaluation limits (for quick CPU evaluation)\n",
    "    eval_ood_max: int = 5000\n",
    "    eval_synth_max: int = 1000\n",
    "\n",
    "    # Synthetic anomaly parameters\n",
    "    synth_prob: float = 1.0\n",
    "    patch_area: Tuple[float, float] = (0.03, 0.14)  # fraction of image area\n",
    "    noise_std: Tuple[float, float] = (0.18, 0.45)\n",
    "\n",
    "    # Score fusion\n",
    "    fuse_w: float = 0.65  # weight for mean recon score in fused rank score\n",
    "\n",
    "    # Localization post-processing\n",
    "    smooth_k: int = 3\n",
    "    quantile_q: float = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4148e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Small utilities\n",
    "\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    \"\"\"Make runs more reproducible (still not perfect, but good enough for CPU eval).\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def _rand_rect(h: int, w: int, area_range: Tuple[float, float]) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Sample a random rectangle (top, left, height, width) given an area range.\n",
    "\n",
    "    - area_range is a fraction of total image area.\n",
    "    - aspect ratio is randomized to avoid only-square patches.\n",
    "    \"\"\"\n",
    "    area = random.uniform(area_range[0], area_range[1]) * (h * w)\n",
    "    aspect = random.uniform(0.6, 1.6)\n",
    "\n",
    "    rh = int(round(math.sqrt(area * aspect)))\n",
    "    rw = int(round(math.sqrt(area / aspect)))\n",
    "\n",
    "    # keep it valid\n",
    "    rh = max(2, min(rh, h - 1))\n",
    "    rw = max(2, min(rw, w - 1))\n",
    "\n",
    "    top = random.randint(0, h - rh)\n",
    "    left = random.randint(0, w - rw)\n",
    "    return top, left, rh, rw\n",
    "\n",
    "\n",
    "def synth_anomaly(x: torch.Tensor, cfg: CFG) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create a synthetic anomaly on a single image tensor x (C,H,W) in [0,1].\n",
    "    Returns:\n",
    "      xa: anomalous image\n",
    "      m : binary anomaly mask (1,H,W) where 1 indicates anomaly region\n",
    "    \"\"\"\n",
    "    c, h, w = x.shape\n",
    "\n",
    "    # Sometimes, do nothing (if synth_prob < 1)\n",
    "    if random.random() > cfg.synth_prob:\n",
    "        m = torch.zeros(1, h, w, dtype=torch.float32)\n",
    "        return x.clone(), m\n",
    "\n",
    "    top, left, rh, rw = _rand_rect(h, w, cfg.patch_area)\n",
    "    m = torch.zeros(1, h, w, dtype=torch.float32)\n",
    "    m[:, top:top + rh, left:left + rw] = 1.0\n",
    "\n",
    "    # Option A: copy-paste a random patch from the same image (harder to detect)\n",
    "    if random.random() < 0.5:\n",
    "        src_top = random.randint(0, h - rh)\n",
    "        src_left = random.randint(0, w - rw)\n",
    "\n",
    "        patch = x[:, src_top:src_top + rh, src_left:src_left + rw].clone()\n",
    "        xa = x.clone()\n",
    "        xa[:, top:top + rh, left:left + rw] = patch\n",
    "        return xa, m\n",
    "\n",
    "    # Option B: add random noise in the patch\n",
    "    std = random.uniform(cfg.noise_std[0], cfg.noise_std[1])\n",
    "    noise = torch.randn((c, rh, rw), dtype=x.dtype) * std\n",
    "\n",
    "    xa = x.clone()\n",
    "    xa[:, top:top + rh, left:left + rw] = torch.clamp(\n",
    "        xa[:, top:top + rh, left:left + rw] + noise, 0.0, 1.0\n",
    "    )\n",
    "    return xa, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d211c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Dataset (One-class CIFAR10)\n",
    "\n",
    "class OneClassCIFAR(Dataset):\n",
    "    \"\"\"\n",
    "    Splits:\n",
    "      - train      : only normal class images, returns x\n",
    "      - test_ood   : full CIFAR10 test, returns (x, y_is_anom)\n",
    "      - test_synth : only normal class test images, returns (x_anom, mask)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, cfg: CFG, download: bool = True):\n",
    "        assert split in {\"train\", \"test_ood\", \"test_synth\"}\n",
    "        self.split = split\n",
    "        self.cfg = cfg\n",
    "\n",
    "        train_flag = split == \"train\"\n",
    "        base = datasets.CIFAR10(root=root, train=train_flag, download=download)\n",
    "\n",
    "        self.data = base.data\n",
    "        self.targets = np.array(base.targets, dtype=np.int64)\n",
    "\n",
    "        # For one-class training (and synthetic eval), keep only normal class\n",
    "        if split in {\"train\", \"test_synth\"}:\n",
    "            idx = np.where(self.targets == cfg.normal_class)[0]\n",
    "            self.data = self.data[idx]\n",
    "            self.targets = self.targets[idx]\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.train_aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomCrop(32, padding=2, padding_mode=\"reflect\"),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        img = TF.to_pil_image(self.data[i])\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            x = self.to_tensor(self.train_aug(img))\n",
    "            return x\n",
    "\n",
    "        if self.split == \"test_ood\":\n",
    "            x = self.to_tensor(img)\n",
    "            y_is_anom = 0 if int(self.targets[i]) == self.cfg.normal_class else 1\n",
    "            return x, torch.tensor(y_is_anom, dtype=torch.long)\n",
    "\n",
    "        if self.split == \"test_synth\":\n",
    "            xc = self.to_tensor(img)\n",
    "            xa, m = synth_anomaly(xc, self.cfg)\n",
    "            return xa, m\n",
    "\n",
    "        raise RuntimeError(\"Invalid split\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a27c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Attention blocks (CBAM)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention via (avg + max) pooled descriptors.\"\"\"\n",
    "    def __init__(self, ch: int, r: int = 8):\n",
    "        super().__init__()\n",
    "        hidden = max(4, ch // r)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(ch, hidden, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, ch, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg = F.adaptive_avg_pool2d(x, 1)\n",
    "        mx = F.adaptive_max_pool2d(x, 1)\n",
    "        w = torch.sigmoid(self.mlp(avg) + self.mlp(mx))\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention via conv over [channel-avg, channel-max].\"\"\"\n",
    "    def __init__(self, k: int = 7):\n",
    "        super().__init__()\n",
    "        p = (k - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=k, padding=p, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        mx, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        a = torch.sigmoid(self.conv(torch.cat([avg, mx], dim=1)))\n",
    "        return x * a, a  # return both modulated features and attention map\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module: Channel + Spatial.\"\"\"\n",
    "    def __init__(self, ch: int, r: int = 8):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(ch, r=r)\n",
    "        self.sa = SpatialAttention(k=7)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.ca(x)\n",
    "        x, a = self.sa(x)\n",
    "        return x, a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c957801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Main model: Conv AutoEncoder + CBAM\n",
    "\n",
    "class ConvAE_CBAM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Conv AutoEncoder with CBAM at bottleneck.\n",
    "    Outputs:\n",
    "      - reconstruction\n",
    "      - attention map (low-res, later upsampled for scoring)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int = 3, base: int = 24, dropout_p: float = 0.10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base, base, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.e2 = nn.Sequential(\n",
    "            nn.Conv2d(base, base * 2, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base * 2, base * 2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.e3 = nn.Sequential(\n",
    "            nn.Conv2d(base * 2, base * 3, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base * 3, base * 3, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck attention + dropout (only during training)\n",
    "        self.cbam = CBAM(base * 3, r=8)\n",
    "        self.drop = nn.Dropout2d(p=dropout_p)\n",
    "\n",
    "        # Decoder\n",
    "        self.d3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base * 3, base * 2, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base * 2, base * 2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.d2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base * 2, base, 4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base, base, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(base, in_ch, 3, padding=1),\n",
    "            nn.Sigmoid(),  # inputs are in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.e1(x)\n",
    "        z = self.e2(z)\n",
    "        z = self.e3(z)\n",
    "\n",
    "        z, att = self.cbam(z)\n",
    "        if self.training:\n",
    "            z = self.drop(z)\n",
    "\n",
    "        y = self.d3(z)\n",
    "        y = self.d2(y)\n",
    "        y = self.out(y)\n",
    "        return y, att\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb60ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Metrics helpers\n",
    "\n",
    "def auroc(scores: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Lightweight AUROC (no sklearn). Assumes labels are {0,1}.\n",
    "    \"\"\"\n",
    "    order = np.argsort(scores)\n",
    "    ranks = np.empty_like(order)\n",
    "    ranks[order] = np.arange(len(scores))\n",
    "\n",
    "    pos = labels == 1\n",
    "    n_pos = int(pos.sum())\n",
    "    n_neg = int((~pos).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    sum_ranks_pos = float(ranks[pos].sum())\n",
    "    return (sum_ranks_pos - n_pos * (n_pos - 1) / 2.0) / (n_pos * n_neg)\n",
    "\n",
    "\n",
    "def rank01(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Turn a vector into rank-based [0,1] values (robust for fusing different scales).\"\"\"\n",
    "    r = torch.argsort(torch.argsort(x)).float()\n",
    "    return r / (r.max() + 1e-8)\n",
    "\n",
    "\n",
    "def iou_dice(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-8) -> Tuple[float, float]:\n",
    "    \"\"\"Compute mean IoU and Dice over a batch. pred/gt expected shape (B,1,H,W).\"\"\"\n",
    "    pred = pred.float()\n",
    "    gt = gt.float()\n",
    "\n",
    "    inter = (pred * gt).sum(dim=(1, 2, 3))\n",
    "    union = (pred + gt - pred * gt).sum(dim=(1, 2, 3))\n",
    "\n",
    "    iou = (inter + eps) / (union + eps)\n",
    "    dice = (2 * inter + eps) / (pred.sum(dim=(1, 2, 3)) + gt.sum(dim=(1, 2, 3)) + eps)\n",
    "    return float(iou.mean().item()), float(dice.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0547af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Evaluation: OOD detection\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ood_scores(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    max_eval: int,\n",
    "    fuse_w: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns AUROC for:\n",
    "      - mean recon error\n",
    "      - attention-weighted recon error\n",
    "      - fused rank score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scores_mean, scores_att, labels = [], [], []\n",
    "    seen = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        xhat, att = model(x)\n",
    "\n",
    "        # Per-pixel recon error (L1)\n",
    "        err = torch.mean(torch.abs(x - xhat), dim=1, keepdim=True)\n",
    "\n",
    "        # Simple image-level score\n",
    "        mean_score = err.mean(dim=(1, 2, 3)).cpu()\n",
    "\n",
    "        # Attention-weighted score (upsample attention to 32x32)\n",
    "        att_up = F.interpolate(att, size=(32, 32), mode=\"bilinear\", align_corners=False)\n",
    "        w_map = 1.0 + att_up\n",
    "        att_score = (err * w_map).sum(dim=(1, 2, 3)) / (w_map.sum(dim=(1, 2, 3)) + 1e-8)\n",
    "        att_score = att_score.cpu()\n",
    "\n",
    "        scores_mean.append(mean_score)\n",
    "        scores_att.append(att_score)\n",
    "        labels.append(y.cpu())\n",
    "\n",
    "        seen += x.size(0)\n",
    "        if seen >= max_eval:\n",
    "            break\n",
    "\n",
    "    s_mean = torch.cat(scores_mean, dim=0)[:max_eval]\n",
    "    s_att = torch.cat(scores_att, dim=0)[:max_eval]\n",
    "    y = torch.cat(labels, dim=0)[:max_eval].numpy()\n",
    "\n",
    "    auc_mean = auroc(s_mean.numpy(), y)\n",
    "    auc_att = auroc(s_att.numpy(), y)\n",
    "\n",
    "    fused = fuse_w * rank01(s_mean) + (1.0 - fuse_w) * rank01(s_att)\n",
    "    auc_fused = auroc(fused.numpy(), y)\n",
    "\n",
    "    return auc_mean, auc_att, auc_fused\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f05bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Evaluation: Synthetic localization\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_synth_localization(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    smooth_k: int,\n",
    "    q: float\n",
    "):\n",
    "    \"\"\"\n",
    "    For synthetic anomalies, compute localization quality:\n",
    "      - threshold 0.5 on normalized anomaly map\n",
    "      - threshold using per-image quantile q\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    iou05, dice05 = [], []\n",
    "    iouq, diceq = [], []\n",
    "\n",
    "    for xa, m in loader:\n",
    "        xa = xa.to(device)\n",
    "        m = m.to(device)\n",
    "\n",
    "        xhat, _ = model(xa)\n",
    "\n",
    "        # Build anomaly map from max channel abs error\n",
    "        a_map = torch.max(torch.abs(xa - xhat), dim=1, keepdim=True)[0]\n",
    "\n",
    "        # Smooth (helps reduce salt-and-pepper noise)\n",
    "        a_map = F.avg_pool2d(a_map, kernel_size=smooth_k, stride=1, padding=smooth_k // 2)\n",
    "\n",
    "        # Normalize to [0,1] per image\n",
    "        a_min = a_map.amin(dim=(1, 2, 3), keepdim=True)\n",
    "        a_max = a_map.amax(dim=(1, 2, 3), keepdim=True)\n",
    "        a_map01 = (a_map - a_min) / (a_max - a_min + 1e-8)\n",
    "\n",
    "        # Fixed threshold\n",
    "        pred05 = (a_map01 > 0.5).float()\n",
    "        i, d = iou_dice(pred05, m)\n",
    "        iou05.append(i)\n",
    "        dice05.append(d)\n",
    "\n",
    "        # Quantile threshold (adapts to each image)\n",
    "        th = torch.quantile(a_map01.flatten(1), q, dim=1).view(-1, 1, 1, 1)\n",
    "        predq = (a_map01 > th).float()\n",
    "        i, d = iou_dice(predq, m)\n",
    "        iouq.append(i)\n",
    "        diceq.append(d)\n",
    "\n",
    "    return (float(np.mean(iou05)), float(np.mean(dice05))), (float(np.mean(iouq)), float(np.mean(diceq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3b511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) DataLoaders\n",
    "\n",
    "def make_loaders(cfg: CFG) -> Dict[str, DataLoader]:\n",
    "    train_ds = OneClassCIFAR(cfg.root, \"train\", cfg, download=True)\n",
    "    ood_ds = OneClassCIFAR(cfg.root, \"test_ood\", cfg, download=True)\n",
    "    synth_ds = OneClassCIFAR(cfg.root, \"test_synth\", cfg, download=True)\n",
    "\n",
    "    # Optional subsampling for faster eval on CPU\n",
    "    if cfg.eval_ood_max and cfg.eval_ood_max < len(ood_ds):\n",
    "        idx = np.random.RandomState(cfg.seed).choice(len(ood_ds), cfg.eval_ood_max, replace=False)\n",
    "        ood_ds = Subset(ood_ds, idx.tolist())\n",
    "\n",
    "    if cfg.eval_synth_max and cfg.eval_synth_max < len(synth_ds):\n",
    "        idx = np.random.RandomState(cfg.seed + 1).choice(len(synth_ds), cfg.eval_synth_max, replace=False)\n",
    "        synth_ds = Subset(synth_ds, idx.tolist())\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.batch_train,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    ood_loader = DataLoader(\n",
    "        ood_ds,\n",
    "        batch_size=cfg.batch_test,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    synth_loader = DataLoader(\n",
    "        synth_ds,\n",
    "        batch_size=cfg.batch_test,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "\n",
    "    return {\"train\": train_loader, \"ood\": ood_loader, \"synth\": synth_loader}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed230a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) DataLoaders\n",
    "def make_loaders(cfg: CFG) -> Dict[str, DataLoader]:\n",
    "    train_ds = OneClassCIFAR(cfg.root, \"train\", cfg, download=True)\n",
    "    ood_ds = OneClassCIFAR(cfg.root, \"test_ood\", cfg, download=True)\n",
    "    synth_ds = OneClassCIFAR(cfg.root, \"test_synth\", cfg, download=True)\n",
    "\n",
    "    # Optional subsampling for faster eval on CPU\n",
    "    if cfg.eval_ood_max and cfg.eval_ood_max < len(ood_ds):\n",
    "        idx = np.random.RandomState(cfg.seed).choice(len(ood_ds), cfg.eval_ood_max, replace=False)\n",
    "        ood_ds = Subset(ood_ds, idx.tolist())\n",
    "\n",
    "    if cfg.eval_synth_max and cfg.eval_synth_max < len(synth_ds):\n",
    "        idx = np.random.RandomState(cfg.seed + 1).choice(len(synth_ds), cfg.eval_synth_max, replace=False)\n",
    "        synth_ds = Subset(synth_ds, idx.tolist())\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.batch_train,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    ood_loader = DataLoader(\n",
    "        ood_ds,\n",
    "        batch_size=cfg.batch_test,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "    synth_loader = DataLoader(\n",
    "        synth_ds,\n",
    "        batch_size=cfg.batch_test,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "    )\n",
    "\n",
    "    return {\"train\": train_loader, \"ood\": ood_loader, \"synth\": synth_loader}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c97b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "loaded: ./runs/best_cpu_fast.pt\n",
      "AUROC recon_mean        : 0.7071\n",
      "AUROC recon_att_weighted: 0.7070\n",
      "AUROC fused(rank)       : 0.7071\n",
      "Localization th=0.5     : IoU 0.4764 | Dice 0.5604\n",
      "Localization quantile   : IoU 0.3476 | Dice 0.4560\n",
      "time: 9.3 s\n",
      "saved visualization: ./runs/vis_final.png\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def save_visual_examples(model, synth_loader, device, out_path, n=8, smooth_k=3):\n",
    "    model.eval()\n",
    "    rows = n\n",
    "    cols = 5\n",
    "\n",
    "    xs, xhats, amaps, preds, gts = [], [], [], [], []\n",
    "\n",
    "    for xa, m in synth_loader:\n",
    "        xa = xa.to(device)\n",
    "        m = m.to(device)\n",
    "\n",
    "        xhat, _ = model(xa)\n",
    "\n",
    "        a_map = torch.max(torch.abs(xa - xhat), dim=1, keepdim=True)[0]\n",
    "        a_map = F.avg_pool2d(a_map, kernel_size=smooth_k, stride=1, padding=smooth_k // 2)\n",
    "\n",
    "        a_map01 = (a_map - a_map.amin(dim=(1, 2, 3), keepdim=True)) / (\n",
    "            a_map.amax(dim=(1, 2, 3), keepdim=True) - a_map.amin(dim=(1, 2, 3), keepdim=True) + 1e-8\n",
    "        )\n",
    "\n",
    "        pred = (a_map01 > 0.5).float()\n",
    "\n",
    "        for i in range(xa.size(0)):\n",
    "            xs.append(xa[i].detach().cpu())\n",
    "            xhats.append(xhat[i].detach().cpu())\n",
    "            amaps.append(a_map01[i].detach().cpu())\n",
    "            preds.append(pred[i].detach().cpu())\n",
    "            gts.append(m[i].detach().cpu())\n",
    "            if len(xs) >= n:\n",
    "                break\n",
    "        if len(xs) >= n:\n",
    "            break\n",
    "\n",
    "    def to_img(x3):\n",
    "        x = x3.permute(1, 2, 0).numpy()\n",
    "        return np.clip(x, 0, 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(cols * 3.2, rows * 3.0))\n",
    "    for r in range(rows):\n",
    "        xa = xs[r]\n",
    "        xhat = xhats[r]\n",
    "        amap = amaps[r][0].numpy()\n",
    "        pm = preds[r][0].numpy()\n",
    "        gm = gts[r][0].numpy()\n",
    "\n",
    "        items = [\n",
    "            (\"Input (synthetic)\", to_img(xa)),\n",
    "            (\"Reconstruction\", to_img(xhat)),\n",
    "            (\"Anomaly map\", amap),\n",
    "            (\"Pred mask\", pm),\n",
    "            (\"GT mask\", gm),\n",
    "        ]\n",
    "\n",
    "        for c in range(cols):\n",
    "            ax = fig.add_subplot(rows, cols, r * cols + c + 1)\n",
    "            title, im = items[c]\n",
    "            if c <= 1:\n",
    "                ax.imshow(im)\n",
    "            else:\n",
    "                ax.imshow(im, cmap=\"gray\", vmin=0, vmax=1)\n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"saved visualization:\", out_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = CFG()\n",
    "    seed_everything(cfg.seed)\n",
    "    torch.set_num_threads(cfg.torch_threads)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    loaders = make_loaders(cfg)\n",
    "\n",
    "    model = ConvAE_CBAM(in_ch=3, base=cfg.base_ch, dropout_p=cfg.dropout_p).to(device)\n",
    "\n",
    "    ckpt_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    print(\"loaded:\", ckpt_path)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    auc_mean, auc_att, auc_fused = eval_ood_scores(\n",
    "        model, loaders[\"ood\"], device, max_eval=cfg.eval_ood_max, fuse_w=cfg.fuse_w\n",
    "    )\n",
    "\n",
    "    (iou05, dice05), (iouq, diceq) = eval_synth_localization(\n",
    "        model, loaders[\"synth\"], device, smooth_k=cfg.smooth_k, q=cfg.quantile_q\n",
    "    )\n",
    "\n",
    "    print(f\"AUROC recon_mean        : {auc_mean:.4f}\")\n",
    "    print(f\"AUROC recon_att_weighted: {auc_att:.4f}\")\n",
    "    print(f\"AUROC fused(rank)       : {auc_fused:.4f}\")\n",
    "    print(f\"Localization th=0.5     : IoU {iou05:.4f} | Dice {dice05:.4f}\")\n",
    "    print(f\"Localization quantile   : IoU {iouq:.4f} | Dice {diceq:.4f}\")\n",
    "    print(\"time:\", round(time.time() - t0, 1), \"s\")\n",
    "\n",
    "    save_visual_examples(\n",
    "        model,\n",
    "        loaders[\"synth\"],\n",
    "        device,\n",
    "        out_path=os.path.join(cfg.save_dir, \"vis_final.png\"),\n",
    "        n=8,\n",
    "        smooth_k=cfg.smooth_k\n",
    "    )\n",
    "\n",
    "# This block ensures that the evaluation pipeline is executed\n",
    "# only when this file is run directly, and not when imported\n",
    "# as a module in another script.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df58f2",
   "metadata": {},
   "source": [
    "## Author:\n",
    "\n",
    "# Rashin Gholijani Farahani - 2025-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd729fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
